name: Comprehensive Testing Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'

jobs:
  # ============================================================================
  # Quick Validation (Fast tests on multiple Python versions)
  # ============================================================================
  quick-validation:
    name: Quick Validation (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-benchmark

      - name: Run fast unit tests
        run: |
          pytest tests/test_model.py tests/test_tokenizer.py -v --tb=short -m "not slow" \
            --benchmark-json benchmark_results.json

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: test-results/

  # ============================================================================
  # Full Test Suite (Comprehensive)
  # ============================================================================
  full-test-suite:
    name: Full Test Suite
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-full-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-full-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-benchmark

      - name: Run all tests with coverage
        run: |
          pytest tests/test_model.py -v --tb=short --cov=model --cov-report=xml:coverage-model.xml
          pytest tests/test_tokenizer.py -v --tb=short --cov=tokenizer --cov-report=xml:coverage-tokenizer.xml
          pytest tests/test_trainer.py -v --tb=short --cov=trainer --cov-report=xml:coverage-trainer.xml
          pytest tests/test_integration.py -v --tb=short
          pytest tests/test_performance.py -v --tb=short --benchmark-json benchmark_results.json

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: coverage-*.xml

  # ============================================================================
  # GPU Tests
  # ============================================================================
  gpu-tests:
    name: GPU Tests
    runs-on: ubuntu-latest
    # For self-hosted GPU runners:
    # runs-on: [self-hosted, gpu]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Check GPU availability
        run: |
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

      - name: Run GPU-specific tests
        run: echo "GPU tests skipped – configure GPU runners to run actual tests"

  # ============================================================================
  # Code Quality
  # ============================================================================
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install linters
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pylint black isort mypy

      - name: Check formatting
        run: black --check . || echo "Black formatting check failed (non-blocking)"
        continue-on-error: true

      - name: Check import sorting
        run: isort --check-only . || echo "Isort check failed (non-blocking)"
        continue-on-error: true

      - name: Lint code
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || echo "Critical flake8 errors found"
          flake8 . --count --exit-zero --max-complexity=15 --max-line-length=120 --statistics
        continue-on-error: true

  # ============================================================================
  # Performance Regression
  # ============================================================================
  performance-regression:
    name: Performance Regression
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install benchmark dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark torch

      - name: Run benchmarks
        run: pytest tests/test_performance.py --benchmark-only --benchmark-json=benchmark_results.json
        continue-on-error: true

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name != 'pull_request'
        with:
          tool: pytest
          output-file-path: benchmark_results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          name: Benchmark
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          skip-fetch-gh-pages: false
          comment-always: false
          summary-always: false
          save-data-file: true
          comment-on-alert: false

  # ============================================================================
  # Summary
  # ============================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [quick-validation, full-test-suite, code-quality]
    if: always()

    steps:
      - name: Show summary
        run: |
          echo "Quick Validation: ${{ needs.quick-validation.result }}"
          echo "Full Test Suite:  ${{ needs.full-test-suite.result }}"
          echo "Code Quality:     ${{ needs.code-quality.result }}"

      - name: Post summary notice
        run: |
          if [ "${{ needs.full-test-suite.result }}" == "success" ]; then
            echo "✅ All critical tests passed!"
          else
            echo "❌ Some tests failed. Please review the logs."
          fi
        continue-on-error: true
