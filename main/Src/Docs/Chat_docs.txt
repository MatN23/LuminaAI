üß† Transformer Chatbot ‚Äî Full Code Documentation

This script loads a fine-tuned Transformer decoder model and provides an interactive command-line chatbot. It supports top-k, nucleus (top-p), and greedy sampling strategies with real-time temperature control.

üì¶ 1. Imports & Setup

import torch, torch.nn as nn, numpy as np, re, math
from pathlib import Path
Torch: Deep learning framework
NumPy: For numerical operations (minor use here)
re: For regex-based text cleaning
math: For computing scaling and encodings
pathlib: For safe file path operations
‚öôÔ∏è 2. Device Selection

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
Selects the best available compute backend:

Apple Silicon (MPS)
CUDA-enabled GPU
CPU fallback
üî¢ 3. Positional Encoding

class PositionalEncoding(nn.Module):
Adds sinusoidal positional info to token embeddings, allowing the transformer to model sequence order.

üß† 4. CharTransformer (Decoder-Only Transformer)

class CharTransformer(nn.Module):
Components:
Embedding: Maps char IDs to hidden vectors
PositionalEncoding: Adds sequence position info
TransformerDecoder: Stack of decoder layers
LayerNorm + Linear: Normalization and output to vocab logits
generate_square_subsequent_mask(sz)
Creates a causal mask to prevent future-token peeking (autoregressive behavior).

üìÇ 5. Model Loader

def load_model(model_path="best_model.pth"):
Loads a saved .pth checkpoint file
Validates necessary metadata keys (char_to_ix, ix_to_char, etc.)
Returns the checkpoint dictionary
üé≤ 6. Sampling Algorithms

Top-k Sampling
def top_k_sampling(probs, k=5):
Selects one of the top-k most likely tokens, based on renormalized probabilities.

Nucleus Sampling (Top-p)
def nucleus_sampling(probs, p=0.9):
Selects the smallest set of tokens whose cumulative probability ‚â• p, then samples from that set.

‚úèÔ∏è 7. Text Generation

def sample_text(...)
Workflow:
Encodes start_str using char_to_ix
Repeatedly feeds input to model to predict next char
Samples next char using chosen strategy
Stops if \n is generated or max_length is reached
Returns decoded output
Supports:
top_k (default)
nucleus / top_p
greedy (argmax)
Custom temperature scaling
üßπ 8. Response Cleaner

def clean_response(response):
Removes special chat tokens (like <|bot|>)
Removes excessive newlines and whitespace
Truncates incomplete sentence fragments at the end
üìã 9. Model Metadata Printer

def print_model_info(checkpoint):
Displays model architecture and training details from the checkpoint:

Vocabulary size
Hidden size
Layers, heads
Final training loss and epoch
üí¨ 10. Main Chat Loop

def main():
Steps:
Loads saved model (best_model.pth)
Initializes the transformer
Provides interactive CLI:
User input ‚Üí prompt
Appends <|user|>{prompt}<|bot|> to context
Calls sample_text() for next response
Cleans and prints response
Updates conversation history
Supported Commands:
Command	Action
exit, quit	Exits the chat
clear	Clears the conversation history
temp X	Sets temperature (e.g., temp 0.8)
topk X	Sets top-k (e.g., topk 10)
nucleus	Switches to nucleus (top-p) sampling
topk_mode	Switches back to top-k sampling
üß† Design Notes

Supports interactive persona-style conversation using special tags like <|user|> and <|bot|>
Keeps chat history, truncating it if it exceeds max_history_length
Handles sampling flexibility via CLI commands
Auto-reloads a checkpointed model trained for character-level inference
‚úÖ Summary

Feature	Description
Model Type	Transformer Decoder (GPT-like)
Input Type	Character-level text
Sampling Methods	Greedy, Top-k, Nucleus
Prompt Format	`<
Output Format	Cleaned natural language response
CLI Interface	Supports commands and real-time tuning
Device Support	Apple MPS, CUDA GPU, or CPU