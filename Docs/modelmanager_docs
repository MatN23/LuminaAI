# Model Manager Documentation

## Overview

The `model_manager.py` module provides a comprehensive model management system for neural transformer models, featuring advanced metadata tracking, LM Studio compatibility, and professional model lifecycle management.

## Features

- **Comprehensive Model Storage**: Save models with rich metadata and training information
- **LM Studio Compatibility**: Automatic conversion to HuggingFace format for use with LM Studio
- **Version Control**: Track model versions, configurations, and performance metrics
- **Export/Import**: Share models across different environments
- **Cleanup Utilities**: Manage disk space and old model versions
- **Cross-Platform Support**: Works on Windows, macOS, and Linux

## Classes

### ModelConfig

Configuration dataclass for transformer model parameters.

**Attributes:**
- `vocab_size` (int): Vocabulary size (default: 32000)
- `hidden_size` (int): Hidden layer size (default: 768)
- `num_layers` (int): Number of transformer layers (default: 8)
- `num_heads` (int): Number of attention heads (default: 8)
- `seq_length` (int): Maximum sequence length (default: 1024)
- `dropout` (float): Dropout rate (default: 0.1)
- `model_type` (str): Model architecture type (default: "WordTransformer")
- `tokenizer_type` (str): Tokenizer type (default: "word")

### TrainingConfig

Configuration dataclass for training parameters.

**Attributes:**
- `learning_rate` (float): Learning rate (default: 3e-4)
- `weight_decay` (float): Weight decay (default: 0.01)
- `batch_size` (int): Training batch size (default: 4)
- `gradient_accumulation_steps` (int): Gradient accumulation steps (default: 8)
- `max_epochs` (int): Maximum training epochs (default: 50)
- `warmup_ratio` (float): Warmup ratio (default: 0.1)
- `save_every` (int): Save checkpoint interval (default: 1000)
- `eval_every` (int): Evaluation interval (default: 500)
- `max_grad_norm` (float): Maximum gradient norm (default: 1.0)
- `label_smoothing` (float): Label smoothing factor (default: 0.1)
- `beta1` (float): Adam beta1 (default: 0.9)
- `beta2` (float): Adam beta2 (default: 0.95)

### ModelMetadata

Comprehensive metadata storage for trained models.

**Attributes:**
- `model_name` (str): Human-readable model name
- `version` (str): Model version string
- `created_at` (str): Creation timestamp
- `last_modified` (str): Last modification timestamp
- `model_config` (ModelConfig): Model configuration
- `training_config` (TrainingConfig): Training configuration
- `dataset_info` (Dict): Dataset information
- `performance_metrics` (Dict): Performance metrics
- `model_size_mb` (float): Model file size in megabytes
- `total_parameters` (int): Total number of parameters
- `trainable_parameters` (int): Number of trainable parameters
- `training_time_hours` (float): Total training time in hours
- `epochs_trained` (int): Number of epochs trained
- `best_loss` (float): Best achieved loss
- `best_perplexity` (float): Best achieved perplexity
- `hardware_used` (str): Hardware information
- `pytorch_version` (str): PyTorch version used
- `cuda_version` (str): CUDA version (if applicable)
- `model_hash` (str): Model hash for integrity
- `tokenizer_hash` (str): Tokenizer hash for integrity
- `notes` (str): Additional notes
- `tags` (List[str]): Model tags

### HuggingFaceCompatibleModel

Wrapper class to make custom models compatible with HuggingFace transformers and LM Studio.

**Methods:**
- `forward()`: Forward pass compatible with HuggingFace interface
- `generate()`: Text generation with temperature, top-k, and top-p sampling

### HuggingFaceCompatibleTokenizer

Wrapper class to make custom tokenizers compatible with HuggingFace transformers.

**Methods:**
- `get_vocab()`: Return vocabulary dictionary
- `_tokenize()`: Tokenize text
- `_convert_token_to_id()`: Convert token to ID
- `_convert_id_to_token()`: Convert ID to token
- `convert_tokens_to_string()`: Convert tokens back to string
- `save_vocabulary()`: Save vocabulary files

### ModelManager

Main model management class.

**Directory Structure:**
```
models/
├── checkpoints/           # Original model format
├── metadata/             # Model metadata JSON files
├── tokenizers/           # Tokenizer files
└── lm_studio/           # LM Studio compatible format
```

## Methods

### ModelManager.__init__(models_dir: str = "models")

Initialize the model manager.

**Parameters:**
- `models_dir`: Base directory for model storage

**Example:**
```python
manager = ModelManager("my_models")
```

### save_model()

Save a model with comprehensive metadata.

**Parameters:**
- `model` (nn.Module): PyTorch model to save
- `tokenizer`: Tokenizer object
- `metadata` (ModelMetadata): Model metadata
- `optimizer` (optional): Optimizer state
- `scheduler` (optional): Scheduler state

**Returns:**
- `str`: Generated model ID

**Example:**
```python
metadata = ModelMetadata(
    model_name="ChatBot v2",
    version="2.1.0",
    created_at=datetime.now().isoformat(),
    model_config=model_config,
    training_config=training_config,
    dataset_info={"source": "custom_dataset", "size": 10000},
    performance_metrics={"loss": 2.45, "perplexity": 11.6},
    # ... other fields
)

model_id = manager.save_model(model, tokenizer, metadata)
```

### load_model()

Load a model with tokenizer and metadata.

**Parameters:**
- `model_id` (str): Model ID or special keywords ('latest', 'best')

**Returns:**
- `Tuple[nn.Module, Any, ModelMetadata]`: Model, tokenizer, and metadata

**Example:**
```python
model, tokenizer, metadata = manager.load_model("best")
```

### list_models()

List all available models with their information.

**Returns:**
- `List[Dict[str, Any]]`: List of model information dictionaries

**Example:**
```python
models = manager.list_models()
for model in models:
    print(f"{model['name']} - Loss: {model['best_loss']:.4f}")
```

### delete_model()

Delete a model and all associated files.

**Parameters:**
- `model_id` (str): Model ID to delete

**Returns:**
- `bool`: Success status

**Example:**
```python
success = manager.delete_model("old_model_id")
```

### export_model()

Export model for sharing or deployment.

**Parameters:**
- `model_id` (str): Model ID to export
- `export_path` (str): Path to export to
- `format` (str): Export format ('original', 'lm_studio', or 'both')

**Returns:**
- `bool`: Success status

**Example:**
```python
manager.export_model("best_model", "./exports/", format="both")
```

### get_lm_studio_models()

Get list of models available in LM Studio format.

**Returns:**
- `List[Dict[str, Any]]`: List of LM Studio compatible models

### convert_existing_to_lm_studio()

Convert an existing model to LM Studio format.

**Parameters:**
- `model_id` (str): Model ID to convert

**Returns:**
- `bool`: Success status

### cleanup_old_models()

Clean up old models, keeping only the best N models.

**Parameters:**
- `keep_best` (int): Number of best models to keep (default: 5)

**Returns:**
- `int`: Number of models deleted

**Example:**
```python
deleted_count = manager.cleanup_old_models(keep_best=3)
```

### print_model_summary()

Print a comprehensive summary of all models.

**Example:**
```python
manager.print_model_summary()
```

## LM Studio Integration

The ModelManager automatically creates LM Studio compatible versions of your models, including:

- **HuggingFace Format**: Compatible model and tokenizer files
- **SafeTensors Support**: Modern weight format preferred by LM Studio
- **Model Cards**: Comprehensive model documentation
- **README Files**: Usage instructions for LM Studio
- **Conversion Scripts**: Scripts to convert to GGUF format

### LM Studio Usage

1. Train and save your model using ModelManager
2. Navigate to the `models/lm_studio/` directory
3. Load the model folder in LM Studio
4. Use the recommended settings from the model card

### Recommended Prompt Format

```
<user> Your question or message here <bot>
```

### Recommended Settings

- **Temperature**: 0.8
- **Top-K**: 50
- **Top-P**: 0.9
- **Repetition Penalty**: 1.1
- **Max Tokens**: 512
- **Stop Tokens**: `</s>`, `<user>`

## File Formats

### Original Format

- `model.pt`: PyTorch state dict with configuration
- `tokenizer.pkl`: Pickled tokenizer object
- `optimizer.pt`: Optimizer state (if saved)
- `scheduler.pt`: Scheduler state (if saved)

### LM Studio Format

- `config.json`: HuggingFace model configuration
- `model.safetensors`: Model weights in SafeTensors format
- `pytorch_model.bin`: Backup PyTorch weights
- `tokenizer.json`: Tokenizer configuration
- `vocab.json`: Vocabulary file
- `model_card.json`: Comprehensive model information
- `README.md`: Usage instructions
- `convert_to_gguf.sh`: GGUF conversion script

### Metadata Format

JSON file containing:
- Model configuration
- Training configuration
- Performance metrics
- Hardware information
- Dataset information
- Hashes for integrity checking

## Error Handling

The ModelManager includes comprehensive error handling:

- **File System Errors**: Graceful handling of missing files/directories
- **Memory Errors**: Automatic cleanup and memory management
- **Corruption Detection**: Hash verification for model integrity
- **Version Compatibility**: Checks for PyTorch and dependency versions

## Best Practices

1. **Consistent Naming**: Use descriptive model names and versions
2. **Regular Cleanup**: Use `cleanup_old_models()` to manage disk space
3. **Backup Important Models**: Export models before major changes
4. **Monitor Performance**: Track loss and perplexity trends
5. **Use Tags**: Tag models for easy organization ('best', 'production', etc.)
6. **Document Changes**: Use the notes field to document model changes

## Dependencies

### Required
- `torch`: PyTorch for model operations
- `numpy`: Numerical operations
- `pathlib`: File system operations

### Optional (for LM Studio compatibility)
- `transformers`: HuggingFace transformers library
- `safetensors`: Modern tensor serialization format

## Installation

```bash
# Required dependencies
pip install torch numpy

# Optional for LM Studio compatibility
pip install transformers safetensors
```

## Usage Examples

### Complete Model Training and Saving

```python
from model_manager import ModelManager, ModelConfig, TrainingConfig, ModelMetadata
from word_transformer import WordTransformer, WordTokenizer
from datetime import datetime

# Initialize components
manager = ModelManager("./models")
tokenizer = WordTokenizer()
model_config = ModelConfig(vocab_size=32000, hidden_size=768)
training_config = TrainingConfig(learning_rate=3e-4, batch_size=8)

# Train your model (implementation specific)
model = WordTransformer(model_config)
# ... training code ...

# Create metadata
metadata = ModelMetadata(
    model_name="Advanced ChatBot",
    version="3.0.0",
    created_at=datetime.now().isoformat(),
    model_config=model_config,
    training_config=training_config,
    dataset_info={
        "source": "custom_conversations",
        "size": 50000,
        "preprocessing": "word_level"
    },
    performance_metrics={
        "final_loss": 1.85,
        "best_loss": 1.82,
        "perplexity": 6.18
    },
    total_parameters=sum(p.numel() for p in model.parameters()),
    training_time_hours=12.5,
    epochs_trained=25,
    best_loss=1.82,
    best_perplexity=6.18,
    hardware_used="NVIDIA RTX 4090",
    pytorch_version=torch.__version__,
    notes="Trained on curated conversation dataset with improved tokenization",
    tags=["best", "production", "v3"]
)

# Save model
model_id = manager.save_model(model, tokenizer, metadata)
print(f"Model saved with ID: {model_id}")

# The model is now available in both original and LM Studio formats
```

### Loading and Using a Model

```python
# Load the best model
model, tokenizer, metadata = manager.load_model("best")

# Use the model for inference
model.eval()
with torch.no_grad():
    input_text = "Hello, how are you?"
    input_ids = tokenizer.encode(input_text)
    output = model.generate(torch.tensor([input_ids]), max_length=100)
    response = tokenizer.decode(output[0].tolist())
    print(f"Response: {response}")
```

### Model Management and Cleanup

```python
# List all models
models = manager.list_models()
for model in models:
    print(f"{model['name']} v{model['version']} - Loss: {model['best_loss']:.4f}")

# Export best model for sharing
manager.export_model("best", "./shared_models/", format="both")

# Clean up old models, keep only top 5
deleted = manager.cleanup_old_models(keep_best=5)
print(f"Deleted {deleted} old models")

# Print comprehensive summary
manager.print_model_summary()
```

## Troubleshooting

### Common Issues

1. **LM Studio compatibility disabled**: Install `transformers` and `safetensors`
2. **Model loading fails**: Check file paths and permissions
3. **Out of memory**: Use `cleanup_old_models()` and check available space
4. **Hash mismatch**: Model file may be corrupted, reload from backup

### Debug Information

Enable debug logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## License

Copyright (c) 2025 Matias Nielsen. All rights reserved.
Licensed under the Custom License.