{
    "_comment": "Fine-tuning Configuration for Ultra Low VRAM Models",
    "_description": "This config file controls the fine-tuning process for models created by train.py",
    
    "base_model_path": "models/model_20250110_143000",
    "model_id": "ultra_low_vram_base",
    "dataset_path": "finetune_data/chat_data.jsonl",
    
    "learning_rate": 3e-5,
    "weight_decay": 0.01,
    "batch_size": 4,
    "gradient_accumulation_steps": 16,
    "max_epochs": 5,
    "warmup_ratio": 0.03,
    "save_every": 500,
    "eval_every": 100,
    "max_grad_norm": 1.0,
    "label_smoothing": 0.1,
    "beta1": 0.9,
    "beta2": 0.95,
    
    "freeze_embeddings": false,
    "freeze_first_n_layers": 0,
    "lora_rank": 0,
    "lora_alpha": 16.0,
    "lora_dropout": 0.1,
    "target_modules": ["qkv", "out_proj", "fc1", "fc2"],
    
    "use_mixed_precision": true,
    "precision_type": "auto",
    "use_loss_scaling": true,
    "amp_opt_level": "O1",
    
    "dataloader_num_workers": 0,
    "pin_memory": false,
    
    "use_deepspeed": true,
    "deepspeed_config": "finetune_deepspeed_config.json",
    "zero_stage": 2,
    
    "max_samples": 5000,
    "validation_split": 0.1,
    
    "_hardware_specific_configs": {
      "cuda_high_vram": {
        "batch_size": 4,
        "gradient_accumulation_steps": 8,
        "max_samples": 10000,
        "freeze_first_n_layers": 0
      },
      
      "cuda_low_vram": {
        "batch_size": 1,
        "gradient_accumulation_steps": 32,
        "max_samples": 3000,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 2
      },
      
      "mps": {
        "learning_rate": 1e-5,
        "batch_size": 1,
        "gradient_accumulation_steps": 32,
        "max_epochs": 3,
        "precision_type": "fp32",
        "use_mixed_precision": false,
        "use_deepspeed": false,
        "max_samples": 1000,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 1
      },
      
      "cpu": {
        "learning_rate": 1e-5,
        "batch_size": 1,
        "gradient_accumulation_steps": 64,
        "max_epochs": 2,
        "warmup_ratio": 0.1,
        "precision_type": "fp32",
        "use_mixed_precision": false,
        "use_deepspeed": false,
        "max_samples": 500,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 3
      }
    },
    
    "_task_specific_configs": {
      "chat_finetuning": {
        "learning_rate": 3e-5,
        "max_epochs": 5,
        "label_smoothing": 0.1,
        "freeze_first_n_layers": 0
      },
      
      "instruction_following": {
        "learning_rate": 2e-5,
        "max_epochs": 3,
        "label_smoothing": 0.05,
        "freeze_embeddings": false,
        "freeze_first_n_layers": 1
      },
      
      "domain_adaptation": {
        "learning_rate": 1e-5,
        "max_epochs": 8,
        "label_smoothing": 0.0,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 3
      },
      
      "lora_finetuning": {
        "learning_rate": 5e-4,
        "lora_rank": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 999,
        "max_epochs": 10
      }
    },
    
    "_data_format_examples": {
      "chat_format": [
        {
          "conversations": [
            {"from": "user", "value": "Hello, how are you?"},
            {"from": "assistant", "value": "I'm doing well, thank you for asking!"}
          ]
        }
      ],
      
      "instruction_format": [
        {
          "instruction": "Explain what machine learning is",
          "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
        }
      ],
      
      "simple_text_format": [
        "<user> What is the capital of France? <assistant> The capital of France is Paris."
      ]
    },
    
    "_usage_instructions": {
      "1_prepare_data": "Place your fine-tuning data in JSONL, JSON, or plain text format",
      "2_set_base_model": "Update 'base_model_path' to point to your trained model directory",
      "3_configure_training": "Adjust learning_rate, batch_size, and other parameters as needed",
      "4_choose_precision": "Set precision_type to 'auto', 'bf16', 'fp16', or 'fp32'",
      "5_layer_freezing": "Use freeze_embeddings and freeze_first_n_layers to reduce memory usage",
      "6_lora_option": "Set lora_rank > 0 to enable LoRA fine-tuning (more memory efficient)",
      "7_run_finetuning": "Execute: python finetune.py"
    },
    
    "_memory_optimization_tips": {
      "ultra_low_vram": {
        "batch_size": 1,
        "gradient_accumulation_steps": 64,
        "freeze_embeddings": true,
        "freeze_first_n_layers": 4,
        "precision_type": "fp16",
        "zero_stage": 3
      },
      
      "low_vram": {
        "batch_size": 2,
        "gradient_accumulation_steps": 32,
        "freeze_first_n_layers": 2,
        "precision_type": "bf16",
        "zero_stage": 2
      },
      
      "medium_vram": {
        "batch_size": 4,
        "gradient_accumulation_steps": 16,
        "freeze_first_n_layers": 1,
        "precision_type": "bf16",
        "zero_stage": 1
      }
    }
  }