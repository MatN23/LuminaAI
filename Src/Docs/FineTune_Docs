# fine_tune.py Documentation

## Overview
fine_tune.py provides specialized fine-tuning capabilities for pre-trained word-level transformer models. It enables domain adaptation, performance improvement, and vocabulary extension using conversation-formatted training data.

## Key Features
- **Base Model Loading**: Extends existing trained models
- **Vocabulary Extension**: Adds new tokens for domain-specific content
- **Conservative Learning**: Optimized learning rates and schedules for fine-tuning
- **Conversation Formatting**: Specialized handling of dialogue data
- **Advanced Scheduling**: Warmup and cosine decay with plateau detection

## Main Components

### 1. Fine-Tuning Dataset

#### `FineTuneDataset`
Specialized dataset class for fine-tuning with conversation formatting:

**Features:**
- Overlapping sequence generation with smaller stride
- Conversation-aware tokenization
- Memory-efficient sequence storage
- Optimized for dialogue data

**Parameters:**
- `texts`: List of conversation texts
- `tokenizer`: WordTokenizer instance
- `seq_length`: Maximum sequence length
- `stride`: Overlap between sequences (seq_length // 4)

### 2. Data Processing Functions

#### `load_fine_tune_data(data_path)`
Loads and processes fine-tuning data with conversation formatting:

**Input Formats:**
- **JSONL**: OASST-style conversation data
- **Plain Text**: Paragraph-based text data

**Processing Features:**
- Role-based conversation formatting (`<user>` and `<bot>` tags)
- Language filtering (English only)
- Quality filtering (deleted entries, review status)
- Conversation pair reconstruction

**Output Format:**
```
<user> What is artificial intelligence? <bot> Artificial intelligence is... </s>
```

### 3. Vocabulary Extension

#### `extend_vocabulary(base_tokenizer, new_texts, max_new_tokens=5000)`
Intelligently extends the base model's vocabulary:

**Process:**
1. Analyzes new training texts
2. Identifies words not in base vocabulary
3. Counts word frequencies
4. Adds most frequent new words (minimum 2 occurrences)

**Returns:**
- Extended tokenizer with new vocabulary
- Count of newly added tokens

#### `extend_model_embeddings(model, old_vocab_size, new_vocab_size)`
Extends model embedding layers for new vocabulary:

**Process:**
1. Creates new embedding layer with extended size
2. Copies old embeddings to new layer
3. Initializes new embeddings with small random values
4. Updates model configuration
5. Maintains weight tying between embeddings and language head

### 4. Advanced Learning Rate Scheduling

#### `WarmupCosineScheduler`
Sophisticated learning rate scheduler for fine-tuning:

**Features:**
- Linear warmup phase
- Cosine annealing decay
- Minimum learning rate protection
- Step-by-step rate adjustment

**Parameters:**
- `warmup_steps`: Steps for linear warmup
- `total_steps`: Total training steps
- `min_lr`: Minimum learning rate (default: 1e-6)

**Formula:**
```python
# Warmup phase
lr = base_lr * current_step / warmup_steps

# Cosine decay phase
progress = (current_step - warmup_steps) / (total_steps - warmup_steps)
lr = min_lr + (base_lr - min_lr) * 0.5 * (1 + cos(Ï€ * progress))
```

### 5. Fine-Tuning Training Loop

#### `fine_tune_epoch(...)`
Optimized training loop for fine-tuning:

**Features:**
- Conservative gradient clipping (max_norm=0.5)
- Gradient accumulation support
- Memory-efficient processing
- Progress logging and monitoring

**Key Optimizations:**
- Lower learning rates for stability
- Careful gradient management
- Regular memory cleanup
- Error recovery for OOM situations

## Usage Examples

### Basic Fine-Tuning
```bash
python fine_tune.py
```

### Interactive Model Selection
```
ðŸ“‹ Available base models:
  1. OASST1_WordTransformer v1.0_epoch_15 (Loss: 2.1234, Size: 45.67MB)
  2. OASST1_WordTransformer v1.0_epoch_20 (Loss: 1.9876, Size: 45.67MB)

Select base model (1-2) or press Enter for best: 1
ðŸŽ¯ Selected base model: OASST1_WordTransformer v1.0_epoch_15
```

### Custom Data Path
```
Enter fine-tuning data path (default: oasst1_data/oasst1_train.jsonl): my_custom_data.jsonl
```

### Training Progress
```
ðŸ”¤ Checking vocabulary coverage...
ðŸ“ˆ Model extended with 1,247 new tokens
ðŸ“š Fine-tuning dataset: 15,432 sequences, 483 batches per epoch
ðŸ“Š Fine-tuning schedule:
  Total steps: 603
  Warmup steps: 30
  Effective batch size: 32

âš¡ Step 1: Use filesystem tool to read the configuration file
âœ… Step 1 completed
âš¡ Step 2: Calculate the optimal learning rate based on model size
âœ… Step 2 completed

ðŸ“Š Fine-tune Epoch 1/10 Summary:
   Loss: 1.8234 | Perplexity: 6.19
   Accuracy: 78.45% | LR: 4.87e-05
   Time: 127.3s | Total: 0.04h

ðŸ’¾ New best fine-tuned model saved: ft_model_20250131_143022
   Loss improvement: 2.1234 â†’ 1.8234 (-0.3000)
```

## Configuration

### Fine-Tuning Parameters
```python
fine_tune_config = TrainingConfig(
    learning_rate=5e-5,        # Much lower than initial training
    weight_decay=0.01,
    batch_size=2,              # Smaller batch size
    gradient_accumulation_steps=16,  # Higher accumulation
    max_epochs=10,             # Fewer epochs
    warmup_ratio=0.05,         # Less warmup needed
    save_every=500,
    eval_every=250,
    max_grad_norm=0.5,         # Conservative clipping
    label_smoothing=0.05,      # Less smoothing
    beta1=0.9,
    beta2=0.95
)
```

### Memory Optimization
- Automatic device selection (CUDA > MPS > CPU)
- Conservative memory usage settings
- Regular garbage collection
- Error recovery for OOM situations

## Advanced Features

### 1. Conversation Reconstruction
The system intelligently reconstructs conversations from message-based data:

```python
# Input: Individual messages with roles
{"role": "prompter", "text": "What is AI?"}
{"role": "assistant", "text": "AI is artificial intelligence..."}

# Output: Formatted conversation
"<user> What is AI? <bot> AI is artificial intelligence... </s>"
```

### 2. Quality Control
Multiple quality filters ensure training data integrity:
- Language filtering (English only)
- Deletion status checking
- Review result validation
- Tree state verification
- Content length validation

### 3. Progressive Model Enhancement
The fine-tuning process progressively improves the model:
- Vocabulary extension for domain coverage
- Parameter fine-tuning for task adaptation
- Learning rate scheduling for stable convergence
- Regular checkpointing for progress preservation

### 4. Comprehensive Metadata
Each fine-tuned model includes detailed metadata:

```python
metadata = ModelMetadata(
    model_name="OASST1_WordTransformer_FineTuned",
    base_model_info="Original model details",
    vocabulary_changes="1,247 tokens added",
    performance_improvements="Loss: 2.12 â†’ 1.82",
    fine_tuning_specifics="Domain adaptation details",
    # ... extensive metadata
)
```

## Technical Architecture

### Model Loading and Extension
1. **Base Model Loading**: Loads pre-trained model and tokenizer
2. **Vocabulary Analysis**: Analyzes fine-tuning data for new tokens
3. **Model Extension**: Extends embeddings if new vocabulary found
4. **Parameter Optimization**: Sets up conservative optimization

### Training Pipeline
1. **Data Loading**: Processes fine-tuning data with conversation formatting
2. **Dataset Creation**: Creates overlapping sequences for training
3. **Training Loop**: Conservative fine-tuning with advanced scheduling
4. **Model Saving**: Saves improved model with comprehensive metadata

### Memory Management
- Device-specific optimizations
- Conservative memory allocation
- Regular cleanup cycles
- OOM recovery mechanisms

## Best Practices

### 1. Base Model Selection
- Choose models with lowest validation loss
- Ensure vocabulary compatibility
- Consider model size vs. improvement potential

### 2. Data Preparation
- Ensure high-quality, domain-relevant data
- Balance conversation types and lengths
- Filter out low-quality interactions

### 3. Hyperparameter Tuning
- Start with provided conservative settings
- Adjust learning rate based on convergence behavior
- Monitor for overfitting with validation data

### 4. Progress Monitoring
- Watch loss improvement trends
- Monitor vocabulary coverage
- Check sample generation quality

## Error Handling

### Common Issues
1. **Base Model Not Found**: Run `python Train.py` first
2. **Data Format Errors**: Ensure JSONL format with required fields
3. **Memory Issues**: Reduce batch size or use CPU mode
4. **Vocabulary Conflicts**: Check tokenizer compatibility

### Recovery Mechanisms
- Automatic fallback to best available model
- Graceful handling of malformed data entries
- Memory cleanup on OOM errors
- Progress preservation during interruptions

## Integration Points

### With Training Pipeline
- Seamless integration with Train.py models
- Compatible with ModelManager system
- Consistent metadata and checkpoint format

### With Chat Interface
- Fine-tuned models work directly with ChatAI.py
- Improved domain-specific responses
- Enhanced vocabulary coverage

## Performance Optimization

### Training Speed
- Gradient accumulation for effective large batch training
- Optimized data loading with minimal workers
- Device-specific optimizations

### Memory Efficiency
- Conservative batch sizes
- Regular memory cleanup
- Efficient sequence generation

### Convergence Quality
- Advanced learning rate scheduling
- Conservative gradient clipping
- Plateau detection and adjustment

This fine-tuning system provides a robust framework for improving pre-trained models with domain-specific data while maintaining training stability and efficiency.